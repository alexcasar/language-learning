{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "\n",
    "#cd ~/Desktop/snet/gits/lang-learn-repo/alex_tests\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import sys  \n",
    "import subprocess\n",
    "import pywt as pywt\n",
    "import itertools\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def PCAmapper(s,data):\n",
    "\n",
    "#def getCleanGrammar(s):\n",
    "\n",
    "#def getRules(s,word):\n",
    "\n",
    "#def getPossibleNextWords: \n",
    "\n",
    "#def main:\n",
    "#  for t in timeStream:\n",
    "#    read data at time t\n",
    "#    group data by clusters\n",
    "#\n",
    "#    for s in signalClusters:\n",
    "#      map data into the pca normaliced space used to 'train' the 'model' (need to see how to do this)\n",
    "#      symbolize based on the location in the pca space\n",
    "#      get the index used in the grammar file to refer to this word\n",
    "#      filter only words that correspond to an anomaly\n",
    "#      readGrammarFile of s\n",
    "#      process the file to have each word and its rules in a easy to use format (need to see how to do this)\n",
    "#\n",
    "#      #this depends on how the grammar works, and i will ask clarification of that with andres\n",
    "#      for a in anomalousWord:\n",
    "#        find it in the processedGrammarFile\n",
    "#        if t = 0:\n",
    "#          get all the disjoints for that word\n",
    "#          initialize disjoints with position 0\n",
    "#        else:\n",
    "#          for d in disjoints:\n",
    "#            check if current word would be at position p of the disjoint\n",
    "#            if it is:\n",
    "#              position = position+1\n",
    "#            else:\n",
    "#              position = 0\n",
    "#            \n",
    "#            if position == maxPosition:\n",
    "#              issue warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruleToPhraseSets(rules):\n",
    "    words = []\n",
    "    for r in rules:\n",
    "        r = r.replace(\"(\",\"\")\n",
    "        r = r.replace(\")\",\"\")\n",
    "        new = r.split(\" & \")\n",
    "        words = words + [new]\n",
    "    words = list(words for words,_ in itertools.groupby(words))\n",
    "    \n",
    "    words2 = []\n",
    "    for w in words:\n",
    "        w2 = list(set(w))\n",
    "        w2.sort()\n",
    "        words2 = words2 + [w2]\n",
    "    \n",
    "    #print(words)\n",
    "    #print(words2)\n",
    "    return words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- es despues de (o que el otro lo tiene a la izquierda)\n",
    "#+ es andes de (o que el otro lo tiene al a derecha)\n",
    "def phrasesFromSets(desc,sets):\n",
    "    size = len(desc[0])\n",
    "    phrases = []\n",
    "    for s in sets:\n",
    "        if len(s)==1:\n",
    "            if(s[0][-1]=='-'):\n",
    "                temp = s[0]\n",
    "                temp = temp.replace('-','')\n",
    "                temp = temp.replace(desc[0],'',1)\n",
    "                phrase = temp + desc[0]\n",
    "            elif(s[0][-1]=='+'):\n",
    "                temp = s[0]\n",
    "                temp = temp.replace('+','')\n",
    "                temp = temp.replace(desc[0],'',1)\n",
    "                phrase = desc[0] + temp\n",
    "            else:\n",
    "                print(\"error, no sign\")\n",
    "        else:\n",
    "            phrase = \"\"\n",
    "            unjoined = []\n",
    "            count = len(s)\n",
    "            for p in s:\n",
    "                if(p[-1]=='-'):\n",
    "                    temp = p[:-1]\n",
    "                    temp = temp.replace(desc[0],'',1)\n",
    "                    sub = temp + desc[0]\n",
    "                elif(p[-1]=='+'):\n",
    "                    temp = p[:-1]\n",
    "                    temp = temp.replace(desc[0],'',1)\n",
    "                    sub = desc[0] + temp\n",
    "                else:\n",
    "                    print(\"error, no sign\")\n",
    "                    \n",
    "                if(count<len(s)):\n",
    "                    if (phrase[:size]==sub[-size:]):\n",
    "                        phrase = sub + phrase[size:]\n",
    "                    elif (sub[:size]==phrase[-size:]):\n",
    "                        phrase = phrase[:size] + sub\n",
    "                    else:\n",
    "                        #-----------#revisar importancia el orden #--------------------------#\n",
    "                        if(p[-1]=='-'):\n",
    "                            phrase = sub + phrase[size:]\n",
    "                            \n",
    "                        elif(p[-1]=='+'):\n",
    "                            phrase = phrase[:size] + sub\n",
    "                            \n",
    "                        #unjoined = unjoined + [sub]\n",
    "                        #print(s,\"error, no join\")\n",
    "                        #-----------#revisar importancia el orden #--------------------------#\n",
    "                else:\n",
    "                    phrase = sub\n",
    "                    \n",
    "                count = count-1\n",
    "        phrases = phrases + [phrase]\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanGrammar(s):  \n",
    "    print(s)\n",
    "    with open(s, 'r') as f:\n",
    "        data = f.readlines()\n",
    " \n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    del data[0]\n",
    "    del data[-1]\n",
    "    del data[-1]\n",
    "    del data[-1]\n",
    "    del data[-1]\n",
    "    \n",
    "    full = len(data)\n",
    "    for r in range(full):\n",
    "        line = data[full-r-1]\n",
    "        if(len(line)==1):\n",
    "            del data[full-r-1]\n",
    "        elif(line[0]==\"%\"):\n",
    "            data[full-r-1] = line[2:-1]\n",
    "        elif(line[0]==\"\\\"\"):\n",
    "            data[full-r-1] = line[1:-3]\n",
    "        else:\n",
    "            data[full-r-1] = line[:-2].split(\" or \")\n",
    "    \n",
    "    clean = []\n",
    "    mapper = {}\n",
    "    for r in range(0,len(data),3): \n",
    "        joined = [data[r],data[r+1]]\n",
    "        mapper[joined[1]]=joined[0]\n",
    "        phraseSets = ruleToPhraseSets(data[r+2])\n",
    "        phrases = phrasesFromSets(joined,phraseSets)\n",
    "        joined = joined + [phrases]\n",
    "        clean = clean + [joined]\n",
    "        \n",
    "    return mapper,clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBadRules(clean,maps,keys,exAnomaly,limit):\n",
    "    size = len(clean[0][0])\n",
    "    maps2 = {}\n",
    "    for m in maps:\n",
    "        maps2[maps[m]]=m\n",
    "    \n",
    "    \n",
    "    clean2 = {}\n",
    "    for c in clean:\n",
    "        clean2[c[0]]=c[2]\n",
    "    \n",
    "    badRules = []\n",
    "    badRuleWord = []\n",
    "    badRuleCounter = []\n",
    "    badRuleWordPos = []\n",
    "    for rule in clean:\n",
    "        if rule[1] in exAnomaly:\n",
    "            badRules = badRules + rule[2]\n",
    "            \n",
    "    badRules = list(set(badRules))\n",
    "    badRules.sort()\n",
    "    badRuleWord = []\n",
    "    badRuleCounter = []\n",
    "    badRuleWordPos = []\n",
    "       \n",
    "    r=0\n",
    "    stop = False\n",
    "    newRules = badRules\n",
    "    tester = 1\n",
    "    while tester != 0:\n",
    "        badRules = list(set(badRules + newRules))\n",
    "        badRules.sort()\n",
    "        newRules = []\n",
    "        stop = True\n",
    "        \n",
    "        tester = len(badRules)\n",
    "        for rule in badRules:\n",
    "            if maps2[rule[:size]] in exAnomaly:\n",
    "                #la primera palabra es anomalia, no hay que agregarle\n",
    "                tester = tester-1\n",
    "            elif len(rule)/size >= limit:\n",
    "                #ya esta muy grande la oracion\n",
    "                tester = tester-1\n",
    "            else:\n",
    "                subRules = []\n",
    "                for c in clean2[rule[:size]]:\n",
    "                    if rule[:size]==c[-size:]:\n",
    "                        subRules = subRules + [c[:-size]+rule]\n",
    "                \n",
    "                if(len(subRules)>0):\n",
    "                    badRules.remove(rule)\n",
    "                    newRules = newRules + subRules\n",
    "                else:\n",
    "                    tester = tester-1\n",
    "    \n",
    "    for rule in badRules:\n",
    "        for anomaly in exAnomaly:\n",
    "            if maps[anomaly] in rule:\n",
    "                badRuleWord = badRuleWord + [maps[anomaly]]\n",
    "                break\n",
    "        badRuleCounter = badRuleCounter + [0]\n",
    "        badRuleWordPos = badRuleWordPos + [rule.find(badRuleWord[r])]\n",
    "        r=r+1\n",
    "        \n",
    "    print(len(badRules))\n",
    "    return badRules, badRuleWord, badRuleCounter, badRuleWordPos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sym0\n",
      "/home/alexcasar/Desktop/snet/gits/lang-learn-repo/alex_tests/data/data11/lang/sym0_PCA_3_3/dict.txt\n",
      "864\n",
      "1153\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-51f9b193684a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0;31m#    badRuleCounter[r] = badRuleCounter[r] + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbadRuleWordPos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbadRuleCounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mbadRuleCounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mbadRuleWordPos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mminW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "limit=10\n",
    "runPredictor=True\n",
    "\n",
    "path = '/home/alexcasar/Desktop/snet/gits/lang-learn-repo/alex_tests/data'\n",
    "filenames = ['data11']\n",
    "filename = filenames[0]\n",
    "doc = pd.read_csv(path+\"/\"+filename+\"/\"+filename+\".csv\") \n",
    "\n",
    "grouped = pd.read_csv(path+\"/\"+filename+\"/\"+filename+\"_PCA_3_3.csv\") \n",
    "grouped['anomaly']=list(doc['anomaly'])\n",
    "groupedAno = grouped[grouped['anomaly']==1]\n",
    "groupedAnoN = grouped[grouped['anomaly']==0]\n",
    "\n",
    "\n",
    "signalClusters=['sym0','sym1','sym2','sym3','sym4','sym5','sym6']\n",
    "for s in signalClusters:\n",
    "    print(s)\n",
    "    langFile = path+\"/\"+filename+\"/lang/\"+s+\"_PCA_3_3/dict.txt\"\n",
    "    maps, clean = getCleanGrammar(langFile)\n",
    "    keys = list(maps.keys())\n",
    "    \n",
    "    anomalies = list(groupedAno.groupby(by=s).count().iloc[:,0].index)\n",
    "    nonanos = list(groupedAnoN.groupby(by=s).count().iloc[:,0].index)\n",
    "    exAnomaly = []\n",
    "    both = []\n",
    "    for a in anomalies:\n",
    "        if a not in nonanos:\n",
    "            exAnomaly = exAnomaly + [a]\n",
    "        else:\n",
    "            both = both + [a]\n",
    "    \n",
    "    badRules, badRuleWord, badRuleCounter, badRuleWordPos = getBadRules(clean,maps,keys,exAnomaly,limit)\n",
    "    \n",
    "    if(runPredictor):\n",
    "        print(len(grouped[s]))\n",
    "        a=0\n",
    "        for word in grouped[s]:\n",
    "            a=a+1\n",
    "            if word in exAnomaly:\n",
    "                print(a,'Anomaly NOW')\n",
    "            else:  \n",
    "                r=0\n",
    "                minW = 999\n",
    "                for rule in badRules:\n",
    "                    if maps[word] in rule:\n",
    "                        badRuleCounter[r] = rule.find(maps[word])\n",
    "                        #if badRuleCounter[r] == -1:\n",
    "                        #    badRuleCounter[r] = rule.find(maps[word])\n",
    "                        #else:\n",
    "                        #    badRuleCounter[r] = badRuleCounter[r] + 1\n",
    "                        pos = badRuleWordPos[r] - badRuleCounter[r]\n",
    "\n",
    "                        if badRuleCounter[r] <= badRuleWordPos[r] and pos < minW:\n",
    "                            minW = pos\n",
    "                    else:\n",
    "                        badRuleCounter[r] = -1\n",
    "                    r = r+1\n",
    "\n",
    "                if minW < 999:# and minW > 0:\n",
    "                    print(a,'Possible anomaly in',minW)\n",
    "                else:\n",
    "                    print(a,'Network is healthy')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
