{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "\n",
    "#cd ~/Desktop/snet/gits/lang-learn-repo/alex_tests\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import sys  \n",
    "import subprocess\n",
    "import pywt as pywt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/Desktop/snet/gits/lang-learn-repo/alex_tests/data'\n",
    "filenames = ['bgpclear_first','bgpclear_second','bgpclear_apptraffic_2hourRun','portflap_first']\n",
    "#dims = 3\n",
    "#blocks = 3\n",
    "limit = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaSymbolize(df,dim=3,block=3):\n",
    "    #divs = blocks**dim\n",
    "    out = df.copy()\n",
    "    \n",
    "    for v in out.columns:\n",
    "        out[v] = (out[v]-min(out[v]))/(max(out[v])-min(out[v]))\n",
    "        \n",
    "    out['sym']=\"R\"\n",
    "    out['num']=0\n",
    "    \n",
    "    for d in range(dim):\n",
    "        out['sym'+str(d)]=0\n",
    "        for b in range(block):\n",
    "            out.loc[out.loc[:,out.columns[d]]>(b+1)/float(block),'sym'+str(d)]=b+1\n",
    "    \n",
    "    for c in range(dim):\n",
    "        out['num']=out['num']+out['sym'+str(c)]*(block**(dim-c-1))\n",
    "    out['sym']=out['sym']+out['num'].astype(str)\n",
    "    \n",
    "    return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaPlot(df,dim=3):\n",
    "    out = df.copy()\n",
    "    \n",
    "    for v in out.columns:\n",
    "        out[v] = (out[v]-min(out[v]))/(max(out[v])-min(out[v]))\n",
    "        \n",
    "    if dim==3:\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.scatter3D(out[out.columns[0]], out[out.columns[1]], out[out.columns[2]], c=out[out.columns[-1]], cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-5-1ce23fd5c7d6>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1ce23fd5c7d6>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    output.to_csv(path+\"/\"+filename+\"/\"+filename+\"_PCA_\"+str(dims)+\"_\"+str(blocks).csv\",index=False)\u001b[0m\n\u001b[0m                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "    #filename = filenames[0]\n",
    "    doc = pd.read_csv(path+\"/\"+filename+\"/all_\"+filename+\"_raw.csv\") \n",
    "    clst = pd.read_csv(path+\"/\"+filename+\"/\"+filename+\"_clusters.csv\")\n",
    "\n",
    "    dataT = doc.copy()\n",
    "\n",
    "    for v in dataT.columns:\n",
    "        dataT[v] = (dataT[v]-min(dataT[v]))/(max(dataT[v])-min(dataT[v]))\n",
    "\n",
    "    dataT = dataT.replace(np.nan,0)\n",
    "    dataT = dataT.T\n",
    "    data = dataT.iloc[:-1,:]\n",
    "    data['clst']=list(clst['cluster'])\n",
    "\n",
    "    for c in range(max(data['clst'])):\n",
    "        sub = data.loc[data.loc[:,'clst']==c,data.columns[:-1]]\n",
    "        sub = sub.T\n",
    "        \n",
    "        #Fitting the PCA algorithm with our Data\n",
    "        pcaT = PCA().fit(sub)\n",
    "        #plt.figure()\n",
    "        #plt.plot(np.cumsum(pcaT.explained_variance_ratio_))\n",
    "        varsum = pd.DataFrame(np.cumsum(pcaT.explained_variance_ratio_))\n",
    "        #dims = min(10,max(3,len(varsum.loc[limit>varsum.loc[:,0],0])))\n",
    "        #blocks = max(3,min(10,int(50/dims)))\n",
    "        dims = 3\n",
    "        blocks = 10\n",
    "        \n",
    "        #plt.xlabel('Number of Components')\n",
    "        #plt.ylabel('Variance (%)') #for each component\n",
    "        #plt.xlim(0,dims+1)\n",
    "        #plt.title('Pulsar Dataset Explained Variance')\n",
    "        #plt.show()\n",
    "        \n",
    "        pca = PCA(n_components=dims)\n",
    "        pca.fit(sub)\n",
    "        newSubs = pca.transform(sub)\n",
    "        newSubs = pd.DataFrame(newSubs)\n",
    "        newSub = pcaSymbolize(newSubs,dims,blocks)\n",
    "\n",
    "        data = data.T\n",
    "        data['sym'+str(c)]=0\n",
    "        data.loc[:len(newSub['sym']),'sym'+str(c)]=list(newSub['sym'])\n",
    "        data = data.T\n",
    "\n",
    "        subput = newSubs.copy()\n",
    "        subput['anomaly']=list(doc['anomaly'])\n",
    "        #pcaPlot(subput)\n",
    "        #display(subput['anomaly'].describe())\n",
    "        \n",
    "    output = data.T\n",
    "    lim = -int(max(data['clst']))\n",
    "    print(lim)\n",
    "    output = output.loc[output.index[:-1],output.columns[lim:]]\n",
    "    display(output)\n",
    "\n",
    "    output.to_csv(path+\"/\"+filename+\"/\"+filename+\"_PCA_\"+str(dims)+\"_\"+str(blocks)+\".csv\",index=False) \n",
    "    output['anomaly']=list(doc['anomaly'])\n",
    "    outSY = output[output['anomaly']==1]\n",
    "    outSN = output[output['anomaly']==0]\n",
    "    \n",
    "    for n in range(len(outSY.columns)):\n",
    "        groupAns = outSY.groupby(by=outSY.columns[n]).count().iloc[:,0].index\n",
    "        print('anomaly:\\t\\t',len(groupAns))\n",
    "        groupNons = outSN.groupby(by=outSY.columns[n]).count().iloc[:,0].index\n",
    "        print('non anomaly\\t\\t',len(groupNons))\n",
    "        print(\"\\ndiffs\")\n",
    "        listG = list(groupAns)+list(groupNons)\n",
    "        setG = set(listG)\n",
    "        print(\"total with rep:\\t\\t\",len(listG))\n",
    "        print(\"total without rep:\\t\",len(setG))\n",
    "        print(\"rep:\\t\\t\\t\",len(listG)-len(setG))\n",
    "        print(\"rep%:\\t\\t\\t\",100*(len(listG)-len(setG))/min(len(groupAns),len(groupNons)),'%')\n",
    "        print(\"rep%:\\t\\t\\t\",100*(1-(len(listG)-len(setG))/len(setG)),'%')\n",
    "        print('------------------------------------------------------')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data.T\n",
    "output = output.loc[output.index[:-1],output.columns[-7:]]\n",
    "output['anomaly']=list(doc['anomaly'])\n",
    "outSY = output[output['anomaly']==1]\n",
    "outSN = output[output['anomaly']==0]\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['anomaly']=list(doc['anomaly'])\n",
    "outSY = output[output['anomaly']==1]\n",
    "outSN = output[output['anomaly']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(outSY.columns)):\n",
    "    print('anomaly')\n",
    "    groupAns = outSY.groupby(by=outSY.columns[n]).count().iloc[:,0].index\n",
    "    print(len(groupAns))\n",
    "    print('non anomaly')\n",
    "    groupNons = outSN.groupby(by=outSY.columns[n]).count().iloc[:,0].index\n",
    "    print(len(groupNons))\n",
    "    print(\"diffs\")\n",
    "    listG = list(groupAns)+list(groupNons)\n",
    "    setG = set(listG)\n",
    "    print(\"total with rep:\\t\\t\",len(listG))\n",
    "    print(\"total without rep:\\t\",len(setG))\n",
    "    print(\"rep:\\t\\t\\t\",len(listG)-len(setG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsts = pd.read_csv(path+\"/\"+filenames[0]+\"/\"+filenames[0]+\"_clusters.csv\")\n",
    "for filename in filenames[1:]:\n",
    "    clst = pd.read_csv(path+\"/\"+filename+\"/\"+filename+\"_clusters.csv\")\n",
    "    clsts = pd.merge(clsts,clst, on='var')\n",
    "clsts.columns = ['var']+filenames\n",
    "clsts = clsts.sort_values(by='var')\n",
    "display(clsts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
